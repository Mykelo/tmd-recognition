{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    preprocess_video,\n",
    "    get_face_detector\n",
    ")\n",
    "\n",
    "from experimental import (\n",
    "    analyze_frontal_video,\n",
    ")\n",
    "\n",
    "from src.detector import PIPNet_PL\n",
    "from src.experiments import WFLW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_front = WFLW.pip_32_16_60_r101_l2_l1_10_1_nb10\n",
    "\n",
    "with HiddenPrints():\n",
    "    face_detector = get_face_detector()\n",
    "    front_lnd_detector = PIPNet_PL(cfg_front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, df, cv=5, reps=20, save_path=None, silent=False, random_state=42):\n",
    "    data = df.drop([\"label\"], axis=1)\n",
    "    X = data.values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    cv = 5\n",
    "    cv_reps = 20\n",
    "    skf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=cv_reps, random_state=random_state)\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    results = {'fold': [], 'f1_macro': [], 'patient_ids': [], 'preds': []}\n",
    "    for fold_idx, (train_index, test_index) in tqdm(enumerate(skf.split(X, y)), total=cv_reps*cv, disable=silent):\n",
    "        id_column = np.where(data.columns == 'patient_id')[0][0]\n",
    "        patient_ids = X[test_index, id_column]\n",
    "        X_ = np.delete(X, id_column, axis=1)\n",
    "\n",
    "        X_train, X_test = X_[train_index], X_[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        scores = []\n",
    "        preds_ = []\n",
    "        for rep in range(reps):\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            score = f1_score(y_test, preds, average=\"macro\")\n",
    "            scores.append(score)\n",
    "            preds_.append(list(preds))\n",
    "\n",
    "        results['fold'].append(fold_idx)\n",
    "        results['f1_macro'].append(np.mean(score))\n",
    "        results['patient_ids'].append(patient_ids)\n",
    "        results['preds'].append(preds_)\n",
    "\n",
    "    if save_path is not None:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_anova_selector(model, df, cv=5, reps=20, save_path=None, silent=False, random_state=42):\n",
    "    data = df.drop([\"label\"], axis=1)\n",
    "    X = data.values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    cv = 5\n",
    "    cv_reps = 20\n",
    "    skf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=cv_reps, random_state=random_state)\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    results = {'fold': [], 'f1_macro': [], 'n_features': [], 'feature_idx': [], 'feature_names': [], 'patient_ids': [], 'preds': []}\n",
    "    for fold_idx, (train_index, test_index) in tqdm(enumerate(skf.split(X, y)), total=cv_reps*cv, disable=silent):\n",
    "        id_column = np.where(data.columns == 'patient_id')[0][0]\n",
    "        patient_ids = X[test_index, id_column]\n",
    "        X_ = np.delete(X, id_column, axis=1)\n",
    "\n",
    "        X_train, X_test = X_[train_index], X_[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        scores = []\n",
    "        preds_ = []\n",
    "        for rep in range(reps):\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            score = f1_score(y_test, preds, average=\"macro\")\n",
    "            scores.append(score)\n",
    "            preds_.append(list(preds))\n",
    "        \n",
    "        feature_idx = model.named_steps['selector'].get_support(indices=True)\n",
    "        feature_mask = model.named_steps['selector'].get_support()\n",
    "        feature_names = list(data.drop(['patient_id'], axis=1).columns[feature_mask].values)\n",
    "\n",
    "        results['fold'].append(fold_idx)\n",
    "        results['f1_macro'].append(np.mean(score))\n",
    "        results['patient_ids'].append(patient_ids)\n",
    "        results['preds'].append(preds_)\n",
    "        results['n_features'].append(model.named_steps['selector'].k)\n",
    "        results['feature_idx'].append(feature_idx)\n",
    "        results['feature_names'].append(feature_names)\n",
    "\n",
    "    if save_path is not None:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def test_rfe_selector(model, df, importance_getter, n_features, cv=5, reps=20, save_path=None, silent=False, random_state=42):\n",
    "    data = df.drop([\"label\"], axis=1)\n",
    "    columns = data.drop('patient_id', axis=1).columns\n",
    "    X = data.values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    cv = 5\n",
    "    cv_reps = 20\n",
    "    skf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=cv_reps, random_state=random_state)\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    results = {'fold': [], 'f1_macro': [], 'n_features': [], 'feature_idx': [], 'feature_names': [], 'patient_ids': [], 'preds': []}\n",
    "    for fold_idx, (train_index, test_index) in tqdm(enumerate(skf.split(X, y)), total=cv_reps*cv, disable=silent):\n",
    "        for k_features in range(1, n_features+1):\n",
    "            id_column = np.where(data.columns == 'patient_id')[0][0]\n",
    "            patient_ids = X[test_index, id_column]\n",
    "            X_ = np.delete(X, id_column, axis=1)\n",
    "\n",
    "            X_train, X_test = X_[train_index], X_[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            sel = RFE(model, importance_getter=f\"named_steps.clf.{importance_getter}\", n_features_to_select=k_features)\n",
    "            sel = sel.fit(X_train, y_train)\n",
    "\n",
    "            feature_idx = [i for i, x in enumerate(sel.support_) if x]\n",
    "            feature_names = list(columns[feature_idx])\n",
    "\n",
    "            X_train_sub = X_train[:, feature_idx]\n",
    "            X_test_sub = X_test[:, feature_idx]\n",
    "\n",
    "            scores = []\n",
    "            preds_ = []\n",
    "            for rep in range(reps):\n",
    "                model.fit(X_train_sub, y_train)\n",
    "                preds = model.predict(X_test_sub)\n",
    "                score = f1_score(y_test, preds, average=\"macro\")\n",
    "                scores.append(score)\n",
    "                preds_.append(list(preds))\n",
    "\n",
    "            results['fold'].append(fold_idx)\n",
    "            results['f1_macro'].append(np.mean(score))\n",
    "            results['patient_ids'].append(patient_ids)\n",
    "            results['preds'].append(preds_)\n",
    "            results['n_features'].append(sel.n_features_)\n",
    "            results['feature_idx'].append(feature_idx)\n",
    "            results['feature_names'].append(feature_names)\n",
    "\n",
    "    if save_path is not None:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_sfs_selector(model, df, sfs_params, n_features, cv=5, cv_reps=5, reps=20, save_path=None, random_state=42):\n",
    "    data = df.drop([\"label\"], axis=1)\n",
    "    feature_names = data.drop('patient_id', axis=1).columns\n",
    "    X = data.values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    cv = 5\n",
    "    cv_reps = 20\n",
    "    skf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=cv_reps, random_state=random_state)\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    results = {'fold': [], 'f1_macro': [], 'n_features': [], 'feature_idx': [], 'feature_names': [], 'patient_ids': [], 'preds': []}\n",
    "    for fold_idx, (train_index, test_index) in tqdm(enumerate(skf.split(X, y)), total=cv_reps*cv):\n",
    "        id_column = np.where(data.columns == 'patient_id')[0][0]\n",
    "        patient_ids = X[test_index, id_column]\n",
    "        X_ = np.delete(X, id_column, axis=1)\n",
    "\n",
    "        X_train, X_test = X_[train_index], X_[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        sel = SFS(**sfs_params, estimator=model, k_features=n_features)\n",
    "        sel = sel.fit(X_train, y_train, custom_feature_names=feature_names)\n",
    "        for k, v in sel.subsets_.items():\n",
    "            X_train_sub = X_train[:, v['feature_idx']]\n",
    "            X_test_sub = X_test[:, v['feature_idx']]\n",
    "\n",
    "            scores = []\n",
    "            preds_ = []\n",
    "            for rep in range(reps):\n",
    "                model.fit(X_train_sub, y_train)\n",
    "                preds = model.predict(X_test_sub)\n",
    "                score = f1_score(y_test, preds, average=\"macro\")\n",
    "                scores.append(score)\n",
    "                preds_.append(list(preds))\n",
    "\n",
    "            results['fold'].append(fold_idx)\n",
    "            results['f1_macro'].append(np.mean(score))\n",
    "            results['n_features'].append(k)\n",
    "            results['feature_idx'].append(list(v['feature_idx']))\n",
    "            results['feature_names'].append(list(v['feature_names']))\n",
    "            results['patient_ids'].append(patient_ids)\n",
    "            results['preds'].append(preds_)\n",
    "\n",
    "    if save_path is not None:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(data_path):\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, data_path))\n",
    "    df_f = df.groupby(\"patient_id\").first()\n",
    "    print(f\"Number of patients: {len(df_f)} (disorder: {len(df_f[df_f['label'] == 1])}, healthy: {len(df_f[df_f['label'] ==0])})\")\n",
    "    print(f\"Number of features: {len(df_f.drop(['label'], axis=1).columns)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_patient_videos(data_path: str, face_detector, front_lnd_detector, prof_lnd_detector, target_size=1000, padding=80, silent=False, video=None):\n",
    "    base_path = data_path.replace(\"patients\", \"preprocessed\")\n",
    "    if video is not None:\n",
    "        vid_name = f\"videos.{video.replace('_', '.')}\"\n",
    "        filenames = list(filter(lambda f: vid_name in f, os.listdir(data_path)))\n",
    "    else:\n",
    "        filenames = list(filter(lambda f: \"videos\" in f, os.listdir(data_path)))\n",
    "\n",
    "    for filename in tqdm(filenames, total=len(filenames), desc=\"Preprocessing patient videos\", disable=silent):\n",
    "        f = os.path.join(data_path, filename)\n",
    "        name = \"_\".join(filename.split(\".\")[1:3])\n",
    "        save_path = os.path.join(base_path, name)\n",
    "        if name.startswith(\"frontal\"):\n",
    "            landmarks_detector = front_lnd_detector\n",
    "        else:\n",
    "            landmarks_detector = prof_lnd_detector\n",
    "        preprocess_video(f, face_detector, landmarks_detector, target_size, padding, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dnz(data_dir, target_size=1000, padding=80, only_new=True):\n",
    "    dirs = os.listdir(data_dir)\n",
    "    dirs = list(filter(lambda x: not x.startswith(\".\"), dirs))\n",
    "    if only_new:\n",
    "        preprocessed = os.listdir(os.path.join(DATA_PATH, \"preprocessed\"))\n",
    "        preprocessed = list(filter(lambda x: not x.startswith(\".\"), preprocessed))\n",
    "        dirs = [dir_ for dir_ in dirs if dir_ not in preprocessed]\n",
    "    for dirname in tqdm(dirs, total=len(dirs), position=0, desc=\"Preprocessing patients\"):\n",
    "        data_path = os.path.join(data_dir, dirname)\n",
    "        preprocess_patient_videos(data_path, face_detector, front_lnd_detector, prof_lnd_detector, target_size, padding, video=\"frontal_open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_path = os.path.join(\"./my_data\", \"patients\")\n",
    "preprocess_dnz(patients_path, only_new=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./my_data/\"\n",
    "patients_path = os.path.join(DATA_PATH, \"patients\")\n",
    "preprocessed_path = os.path.join(DATA_PATH, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diagnosis():\n",
    "    diagnosis = pd.read_csv(os.path.join(DATA_PATH, \"diagnosis.csv\"))\n",
    "    diagnosis = diagnosis.dropna().reset_index(drop=True)\n",
    "    return diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_score(random_state=42):\n",
    "    features = pd.read_csv(os.path.join(DATA_PATH, f\"features.csv\"))\n",
    "    features_df = ex3_data[ex3_data[\"video\"] == str([\"frontal_open\"])]\n",
    "    random_model = DummyClassifier(strategy=\"stratified\", random_state=random_state)\n",
    "    results = test_model(random_model, features_df, silent=True)\n",
    "    return np.mean(results[\"f1_macro\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex1(df_, figsize=None):\n",
    "    df = df_.copy()\n",
    "    df[\"label\"] = df.apply(lambda x: \"line_\" + x[\"line\"] + \"_cp_\" + str(x[\"central_point\"]), axis=1)\n",
    "    df.drop([\"line\", \"central_point\", \"fold\", \"preds\", \"patient_ids\"], inplace=True, axis=1)\n",
    "\n",
    "    groups = df.groupby(\"label\")\n",
    "    values = groups[\"f1_macro\"].mean().values\n",
    "    labels = groups.mean().index.values\n",
    "\n",
    "    best_5_idx = (-values).argsort()[:5]\n",
    "    print(\"5 best socres:\")\n",
    "    for idx in best_5_idx:\n",
    "        print(f\"{labels[idx]}: {values[idx]}\")\n",
    "\n",
    "    if figsize is not None:\n",
    "        f, ax = plt.subplots(figsize=figsize)\n",
    "    plt.bar(labels, values)\n",
    "    plt.grid(axis='y', zorder=0)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel(\"Mean score (f1_macro)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ex1_features(data_dir: str, save_name: str, diagnosis, videos, line_v=\"v6\", line_h=\"h1\", central_point=16, silent=False):\n",
    "    dirs = FINAL_IDS\n",
    "    extractors = {\n",
    "        \"frontal_open\": analyze_frontal_video,\n",
    "    }\n",
    "\n",
    "    patients = []\n",
    "    for patient_id in tqdm(dirs, total=len(dirs), position=0, desc=\"Extracting patients features\", disable=silent):\n",
    "        features = {}\n",
    "        for video in videos:\n",
    "            extractor = extractors[video]\n",
    "            patient_path = os.path.join(data_dir, patient_id, video)\n",
    "            vid_features = extractor(patient_path, line_v=line_v, line_h=line_h, central_point=central_point)\n",
    "            for k, v in vid_features.items():\n",
    "                features[f\"{video}_{k}\"] = v\n",
    "\n",
    "        label = None\n",
    "        diagnosed_ids = list(diagnosis[\"Id\"])\n",
    "        if patient_id in diagnosed_ids:\n",
    "            diag = diagnosis[diagnosis[\"Id\"] == patient_id][\"Diagnosis\"].values[0]\n",
    "            label = 0 if diag == \"healthy\" else 1\n",
    "\n",
    "        features[\"patient_id\"] = patient_id\n",
    "        features[\"label\"] = label\n",
    "        patients.append(features)\n",
    "\n",
    "    save_path = os.path.join(data_dir, \"..\", save_name)\n",
    "    df = pd.DataFrame.from_dict(patients)\n",
    "    df.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_name = \"ex1_data\"\n",
    "videos = [\"frontal_open\"]\n",
    "\n",
    "central_points = [16, 85, 94, [94, 85], [16, 85, 94], [93, 94, 95], [86, 85, 84], [86, 85, 84] + [95, 94, 93]]\n",
    "v_lines = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\"]\n",
    "h_lines = [\"h1\", \"h2\", \"h3\"]\n",
    "\n",
    "for line_h in h_lines:\n",
    "    df = pd.DataFrame()\n",
    "    options = list(itertools.product(v_lines, central_points))\n",
    "    for line_v, central_point in tqdm(options, total=len(options), desc=f\"Experiment 1: extraction for {line_h}\"):\n",
    "        save_name = f\"{ex_name}_{line_h}\"\n",
    "        extract_ex1_features(preprocessed_path, save_name, get_diagnosis(), videos, line_v=line_v, line_h=line_h, central_point=central_point, silent=True)\n",
    "        t_df = pd.read_csv(os.path.join(DATA_PATH, f\"{save_name}.csv\"))\n",
    "        t_df = t_df.dropna().reset_index(drop=True)\n",
    "        t_df[\"label\"] = t_df[\"label\"].astype('int')\n",
    "        t_df[\"line\"] = line_v\n",
    "        t_df[\"central_point\"] = str(central_point)\n",
    "        df = pd.concat([df, t_df], ignore_index=True)\n",
    "\n",
    "    save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "    df.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_name = \"ex1_res\"\n",
    "\n",
    "for line_h in h_lines:\n",
    "    ex1_data = pd.read_csv(os.path.join(DATA_PATH, f\"ex1_data_{line_h}.csv\"))\n",
    "    ex1_res = pd.DataFrame()\n",
    "\n",
    "    lines = list(ex1_data[\"line\"].unique())\n",
    "    central_points = list(ex1_data[\"central_point\"].unique())\n",
    "\n",
    "    options = list(itertools.product(lines, central_points))\n",
    "    for line, central_point in tqdm(options, total=len(options), desc=f\"Experiment 1: results for {line_h}\"):\n",
    "        t_df = ex1_data[(ex1_data[\"line\"] == line) & (ex1_data[\"central_point\"] == central_point)]\n",
    "        t_df = t_df.drop([\"line\", \"central_point\"], axis=1)\n",
    "\n",
    "        scaler = StandardScaler\n",
    "        svc_pipe = Pipeline([\n",
    "            ('scaler', scaler()),\n",
    "            ('svc', SVC(kernel=\"linear\"))\n",
    "        ])\n",
    "        results = test_model(svc_pipe, t_df, reps=1, silent=True)\n",
    "        res_df = pd.DataFrame(results)\n",
    "        res_df[\"line\"] = line\n",
    "        res_df[\"central_point\"] = central_point\n",
    "        ex1_res = pd.concat([ex1_res, res_df], ignore_index=True)\n",
    "\n",
    "    save_name = f\"{ex_name}_{line_h}\"\n",
    "    save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "    ex1_res.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_info(\"ex1_data_h1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex1(df_, line_h, save_path=None, figsize=None):\n",
    "    df = df_.copy()\n",
    "    df[\"label\"] = df.apply(lambda x: \"line_\" + x[\"line\"] + \"_cp_\" + str(x[\"central_point\"]), axis=1)\n",
    "    df.drop([\"line\", \"central_point\", \"fold\", \"preds\", \"patient_ids\"], inplace=True, axis=1)\n",
    "\n",
    "    groups = df.groupby(\"label\")\n",
    "    values = groups[\"f1_macro\"].mean().values\n",
    "    stds = groups[\"f1_macro\"].std().values\n",
    "    labels = groups.mean().index.values\n",
    "\n",
    "    label_to_values = dict(zip(labels, values))\n",
    "    label_to_std = dict(zip(labels, stds))\n",
    "\n",
    "    best_5_idx = (-values).argsort()[:5]\n",
    "    print(\"5 best socres:\")\n",
    "    for idx in best_5_idx:\n",
    "        print(f\"{labels[idx]}: {values[idx]}\")\n",
    "\n",
    "    lines = np.unique([name.split(\"_\")[1] for name in labels])\n",
    "    points = np.unique([name.split(\"_\")[-1] for name in labels])\n",
    "    data_df = pd.DataFrame([[line] + [label_to_values[f'line_{line}_cp_{point}'] for point in points] for line in lines], columns=['Central line'] + [f'Point {i+1}' for i in range(len(points))])\n",
    "\n",
    "    data_df.plot(\n",
    "        x='Central line',\n",
    "        kind='bar',\n",
    "        title=f\"Results depending on the observed point and the determined centerline\",\n",
    "        zorder=2,\n",
    "        width=0.75,\n",
    "        figsize=figsize\n",
    "    )\n",
    "    plt.ylabel(\"F1 mesaure (macro)\")\n",
    "    plt.grid(axis='y', zorder=1)\n",
    "    plt.xticks(rotation='horizontal')\n",
    "    plt.axhline(y=get_random_score(), color='r', linestyle='dashed', label=\"Random prediction\")\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0)\n",
    "    plt.tight_layout(rect=[0,0,0.75,1])\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "\n",
    "    data_df = pd.DataFrame([[line] + [label_to_values[f'line_{line}_cp_{point}'] for point in points] + [label_to_std[f'line_{line}_cp_{point}'] for point in points] for line in lines], columns=['Linia pośrodkowa'] + [f'Punkt {i+1} mu' for i in range(len(points))] + [f'Punkt {i+1} std' for i in range(len(points))])\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_df = pd.DataFrame()\n",
    "\n",
    "for line_h in h_lines:\n",
    "    print(f\"=== LINE H: {line_h} ===\")\n",
    "    save_name = f\"ex1_res_{line_h}\"\n",
    "    save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "    ex1_res = pd.read_csv(f\"{save_path}.csv\")\n",
    "\n",
    "    save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex1_{line_h}\")\n",
    "\n",
    "    temp_df = plot_ex1(ex1_res, line_h, figsize=(15, 5), save_path=save_path)\n",
    "    temp_df = temp_df.rename(columns={\"Linia pośrodkowa\": \"Linia v\"})\n",
    "    temp_df[\"Linia h\"] = line_h\n",
    "    ex1_df = pd.concat([ex1_df, temp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex2(df_, bbox_to_anchor, save_path=None, figsize=None):\n",
    "    df = df_.copy()\n",
    "    df.drop([\"fold\", \"preds\", \"patient_ids\"], inplace=True, axis=1)\n",
    "\n",
    "    groups = df.groupby(\"model\")\n",
    "    values = groups[\"f1_macro\"].mean().values\n",
    "    stds = groups[\"f1_macro\"].std().values\n",
    "    labels = groups.mean().index.values\n",
    "\n",
    "    sorted_values_idx = (-values).argsort()\n",
    "    sorted_labels = [labels[idx] for idx in sorted_values_idx]\n",
    "    sorted_values = [values[idx] for idx in sorted_values_idx]\n",
    "    sorted_stds = [stds[idx] for idx in sorted_values_idx]\n",
    "\n",
    "    best_5_idx = (-values).argsort()[:5]\n",
    "    print(\"5 best socres:\")\n",
    "    for idx in best_5_idx:\n",
    "        print(f\"{labels[idx]}: {values[idx]}\")\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Results depending on the model\")\n",
    "    plt.bar(sorted_labels, sorted_values, zorder=2, yerr=sorted_stds, capsize=5)\n",
    "    plt.grid(axis='y', zorder=1)\n",
    "    plt.ylabel(\"F1 measure (macro)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.axhline(y=get_random_score(), color='r', linestyle='dashed', label=\"Random prediction\")\n",
    "    plt.legend(bbox_to_anchor=bbox_to_anchor, borderaxespad=0)\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ex2_features(data_dir: str, save_name: str, diagnosis, videos, lowpass_kernel, line_v=\"v4\", line_h=\"h1\", central_point=16, silent=False):\n",
    "    dirs = FINAL_IDS\n",
    "    extractors = {\n",
    "        \"frontal_open\": analyze_frontal_video,\n",
    "    }\n",
    "\n",
    "    patients = []\n",
    "    for patient_id in tqdm(dirs, total=len(dirs), position=0, desc=\"Extracting patients features\", disable=silent):\n",
    "        features = {}\n",
    "        for video in videos:\n",
    "            extractor = extractors[video]\n",
    "            patient_path = os.path.join(data_dir, patient_id, video)\n",
    "            vid_features = extractor(patient_path, line_v=line_v, line_h=line_h, central_point=central_point, lowpass_kernel=lowpass_kernel)\n",
    "            for k, v in vid_features.items():\n",
    "                features[f\"{video}_{k}\"] = v\n",
    "\n",
    "        label = None\n",
    "        diagnosed_ids = list(diagnosis[\"Id\"])\n",
    "        if patient_id in diagnosed_ids:\n",
    "            diag = diagnosis[diagnosis[\"Id\"] == patient_id][\"Diagnosis\"].values[0]\n",
    "            label = 0 if diag == \"healthy\" else 1\n",
    "\n",
    "        features[\"patient_id\"] = patient_id\n",
    "        features[\"label\"] = label\n",
    "        patients.append(features)\n",
    "\n",
    "    save_path = os.path.join(data_dir, \"..\", save_name)\n",
    "    df = pd.DataFrame.from_dict(patients)\n",
    "    df.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"ex2_data\"\n",
    "\n",
    "central_point = 85\n",
    "line_v =  \"v8\"\n",
    "line_h = \"h1\"\n",
    "lowpass_kernel = None\n",
    "videos = [\"frontal_open\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "extract_ex2_features(preprocessed_path, save_name, get_diagnosis(), videos, lowpass_kernel=lowpass_kernel, line_v=line_v, line_h=line_h, central_point=central_point, silent=False)\n",
    "t_df = pd.read_csv(os.path.join(DATA_PATH, f\"{save_name}.csv\"))\n",
    "t_df = t_df.dropna().dropna().reset_index(drop=True)\n",
    "t_df[\"label\"] = t_df[\"label\"].astype('int')\n",
    "df = pd.concat([df, t_df], ignore_index=True)\n",
    "\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "df.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ex2_data = pd.read_csv(os.path.join(DATA_PATH, f\"ex2_data.csv\"))\n",
    "ex2_res = pd.DataFrame()\n",
    "\n",
    "models = {\n",
    "    \"prior\": DummyClassifier(strategy=\"prior\"),\n",
    "    \"RLR\": LogisticRegression(),\n",
    "    \"SVC\": SVC(kernel=\"linear\"),\n",
    "    \"XGB\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "options = list(models.items())\n",
    "for model_name, model in tqdm(options, total=len(options), desc=\"Experiment 4: results\"):\n",
    "    t_df = ex2_data\n",
    "\n",
    "    scaler = StandardScaler\n",
    "    model_pipe = Pipeline([\n",
    "        ('scaler', scaler()),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    results = test_model(model_pipe, t_df, reps=1, silent=True)\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df[\"model\"] = model_name\n",
    "    ex2_res = pd.concat([ex2_res, res_df], ignore_index=True)\n",
    "\n",
    "save_name = \"ex2_res\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex2_res.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_info(\"ex2_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"ex2_res\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex2_res = pd.read_csv(f\"{save_path}.csv\")\n",
    "\n",
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex2\")\n",
    "plot_ex2(ex2_res, bbox_to_anchor=(1.155, 1), figsize=(15, 5), save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ['']\n",
    "keys = ['model']\n",
    "key_name = 'Model'\n",
    "\n",
    "index_order = ex2_res.groupby('model')[\"f1_macro\"].mean().sort_values(ascending=False).index.to_list()\n",
    "columns = ['RLR', 'XGB', 'SVC', 'Prior']\n",
    "print(index_order)\n",
    "\n",
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex2.tex\")\n",
    "to_latex_table(ex2_res, keys=keys, key_name=key_name, rows=rows, columns=columns, save_path=save_path, index_order=index_order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ex3_features(data_dir: str, save_name: str, diagnosis, videos, lowpass_kernel, line_v=\"v4\", line_h=\"h1\", central_point=16, silent=False):\n",
    "    dirs = FINAL_IDS\n",
    "    extractors = {\n",
    "        \"frontal_open\": analyze_frontal_video,\n",
    "    }\n",
    "\n",
    "    patients = []\n",
    "    for patient_id in tqdm(dirs, total=len(dirs), position=0, desc=\"Extracting patients features\", disable=silent):\n",
    "        features = {}\n",
    "        for video in videos:\n",
    "            extractor = extractors[video]\n",
    "            patient_path = os.path.join(data_dir, patient_id, video)\n",
    "            vid_features = extractor(patient_path, line_v=line_v, line_h=line_h, central_point=central_point, lowpass_kernel=lowpass_kernel)\n",
    "            for k, v in vid_features.items():\n",
    "                features[f\"{video}_{k}\"] = v\n",
    "\n",
    "        label = None\n",
    "        diagnosed_ids = list(diagnosis[\"Id\"])\n",
    "        if patient_id in diagnosed_ids:\n",
    "            diag = diagnosis[diagnosis[\"Id\"] == patient_id][\"Diagnosis\"].values[0]\n",
    "            label = 0 if diag == \"healthy\" else 1\n",
    "\n",
    "        features[\"patient_id\"] = patient_id\n",
    "        features[\"label\"] = label\n",
    "        patients.append(features)\n",
    "\n",
    "    save_path = os.path.join(data_dir, \"..\", save_name)\n",
    "    df = pd.DataFrame.from_dict(patients)\n",
    "    df.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"ex3_data\"\n",
    "central_point = 85\n",
    "line_v =  \"v8\"\n",
    "line_h = \"h1\"\n",
    "lowpass_kernel = None\n",
    "videos = [\"frontal_open\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "extract_ex3_features(preprocessed_path, save_name, get_diagnosis(), videos, lowpass_kernel=lowpass_kernel, line_v=line_v, line_h=line_h, central_point=central_point, silent=False)\n",
    "t_df = pd.read_csv(os.path.join(DATA_PATH, f\"{save_name}.csv\"))\n",
    "t_df = t_df.dropna().dropna().reset_index(drop=True)\n",
    "t_df[\"label\"] = t_df[\"label\"].astype('int')\n",
    "df = pd.concat([df, t_df], ignore_index=True)\n",
    "\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "df.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3_data = pd.read_csv(os.path.join(DATA_PATH, f\"ex3_data.csv\"))\n",
    "ex3_res = pd.DataFrame()\n",
    "\n",
    "max_features = 22\n",
    "scaler = StandardScaler\n",
    "\n",
    "for n_features in tqdm(range(1, max_features+1), total=max_features, desc=\"Experiment 6: results for ANOVA\"):\n",
    "    t_df = ex3_data\n",
    "\n",
    "    # === ANOVA ===\n",
    "    scaler = StandardScaler\n",
    "    model_pipe = Pipeline([\n",
    "        ('scaler', scaler()),\n",
    "        ('selector', SelectKBest(k=n_features)),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "    results = test_anova_selector(model_pipe, t_df, silent=True, reps=1)\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df[\"n_features\"] = n_features\n",
    "    ex3_res = pd.concat([ex3_res, res_df], ignore_index=True)\n",
    "\n",
    "save_name = \"ex3_res_anova\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 22\n",
    "scaler = StandardScaler\n",
    "\n",
    "sfs_params = {'forward': True, 'floating': False, 'verbose': 0, 'scoring': 'f1_macro', 'cv': 5} \n",
    "ex3_data = pd.read_csv(os.path.join(DATA_PATH, f\"ex3_data.csv\"))\n",
    "ex3_res = pd.DataFrame()\n",
    "\n",
    "# === ANOVA ===\n",
    "t_df = ex3_data\n",
    "svc_pipe = Pipeline([\n",
    "    ('scaler', scaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "results  = test_sfs_selector(svc_pipe, t_df, sfs_params, n_features=max_features, reps=1)\n",
    "res_df = pd.DataFrame(results)\n",
    "ex3_res = pd.concat([ex3_res, res_df], ignore_index=True)\n",
    "\n",
    "save_name = \"ex3_res_sfs\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 22\n",
    "sfs_params = {'forward': True, 'floating': False, 'verbose': 0, 'scoring': 'f1_macro', 'cv': 5} \n",
    "scaler = StandardScaler\n",
    "\n",
    "ex3_data = pd.read_csv(os.path.join(DATA_PATH, f\"ex3_data.csv\"))\n",
    "ex3_res = pd.DataFrame()\n",
    "\n",
    "# === SFS ===\n",
    "t_df = ex3_data\n",
    "svc_pipe = Pipeline([\n",
    "    ('scaler', scaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "results  = test_sfs_selector(svc_pipe, t_df, sfs_params, n_features=max_features, reps=1)\n",
    "res_df = pd.DataFrame(results)\n",
    "ex3_res = pd.concat([ex3_res, res_df], ignore_index=True)\n",
    "\n",
    "save_name = \"ex3_res_sfs\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 22\n",
    "scaler = StandardScaler\n",
    "\n",
    "ex3_data = pd.read_csv(os.path.join(DATA_PATH, f\"ex3_data.csv\"))\n",
    "ex3_res = pd.DataFrame()\n",
    "\n",
    "# === RFE ===\n",
    "t_df = ex3_data\n",
    "svc_pipe = Pipeline([\n",
    "    ('scaler', scaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "results  = test_rfe_selector(svc_pipe, t_df, 'coef_', n_features=max_features, reps=1)\n",
    "res_df = pd.DataFrame(results)\n",
    "ex3_res = pd.concat([ex3_res, res_df], ignore_index=True)\n",
    "\n",
    "save_name = \"ex3_res_rfe\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res.to_csv(f\"{save_path}.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'ex3_res_anova'\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res = pd.read_csv(f\"{save_path}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"ex3_res_sfs\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res = pd.read_csv(f\"{save_path}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"ex3_res_rfe\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex3_res = pd.read_csv(f\"{save_path}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ex3(bbox_to_anchor=(1.155, 1), figsize=(15, 5), save_path=save_path):\n",
    "    anova_df = pd.read_csv(os.path.join(preprocessed_path, '..', 'ex3_res_anova.csv'))\n",
    "    rfe_df = pd.read_csv(os.path.join(preprocessed_path, '..', 'ex3_res_rfe.csv'))\n",
    "    sfs_df = pd.read_csv(os.path.join(preprocessed_path, '..', 'ex3_res_sfs.csv'))\n",
    "\n",
    "    anova_scores = []\n",
    "    rfe_scores = []\n",
    "    sfs_scores = []\n",
    "\n",
    "    n_features_range = anova_df[\"n_features\"].unique()\n",
    "    for n_features in n_features_range:\n",
    "        anova_score = np.mean(anova_df[anova_df[\"n_features\"] == n_features][\"f1_macro\"])\n",
    "        rfe_score = np.mean(rfe_df[rfe_df[\"n_features\"] == n_features][\"f1_macro\"])\n",
    "        sfs_score = np.mean(sfs_df[sfs_df[\"n_features\"] == n_features][\"f1_macro\"])\n",
    "        anova_scores.append(anova_score)\n",
    "        rfe_scores.append(rfe_score)\n",
    "        sfs_scores.append(sfs_score)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Results depending on the number of features selected by various selectors\")\n",
    "    xticks = np.arange(1, len(n_features_range)+1, step=1)\n",
    "    plt.plot(xticks, anova_scores, '-o', markersize=5, label=\"ANOVA\", zorder=2)\n",
    "    plt.plot(xticks, rfe_scores, '-o', markersize=5, label=\"RFE\", zorder=2)\n",
    "    plt.plot(xticks, sfs_scores, '-o', markersize=5, label=\"SFS\", zorder=2)\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"F1 measure (macro)\")\n",
    "    plt.xticks(xticks)\n",
    "    plt.grid(axis='y', zorder=1)\n",
    "    plt.axhline(y=get_random_score(), color='r', linestyle='dashed', label=\"Random prediction\")\n",
    "    plt.legend(bbox_to_anchor=bbox_to_anchor, borderaxespad=0)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"ex2_frontal_res\"\n",
    "save_path = os.path.join(preprocessed_path, \"..\", save_name)\n",
    "ex2_frontal_res = pd.read_csv(f\"{save_path}.csv\")\n",
    "\n",
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex3\")\n",
    "plot_ex3(bbox_to_anchor=(1.155, 1), figsize=(15, 5), save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [str(x) for x in range(1, 23)]\n",
    "keys = ['n_features', 'selector']\n",
    "key_name = 'Selektor'\n",
    "columns = [\"ANOVA\", \"RFE\", \"SFS\"]\n",
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex3.tex\")\n",
    "\n",
    "anova_table_df = pd.read_csv(os.path.join(preprocessed_path, '..', 'ex3_res_anova.csv'))\n",
    "rfe_table_df = pd.read_csv(os.path.join(preprocessed_path, '..', 'ex3_res_rfe.csv'))\n",
    "sfs_table_df = pd.read_csv(os.path.join(preprocessed_path, '..', 'ex3_res_sfs.csv'))\n",
    "\n",
    "anova_table_df[\"selector\"] = \"ANOVA\"\n",
    "rfe_table_df[\"selector\"] = \"RFE\"\n",
    "sfs_table_df[\"selector\"] = \"SFS\"\n",
    "\n",
    "ex3_to_table = pd.concat([anova_table_df, rfe_table_df, sfs_table_df])\n",
    "\n",
    "#  keys order -> row, column\n",
    "to_latex_table(ex3_to_table, keys=keys, key_name=key_name, rows=rows, columns=columns, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def plot_importances(df, selector, n_features, save_path=None, figsize=None):\n",
    "    df[\"feature_names\"] = df[\"feature_names\"].apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "    feature_names = df[df[\"n_features\"] == n_features][\"feature_names\"].values\n",
    "    feature_names = flatten(feature_names)\n",
    "    ctr = Counter(feature_names)\n",
    "    ctr_sorted = ctr.most_common()\n",
    "\n",
    "    keys = [k for (k, v) in ctr_sorted]\n",
    "    values = [v for (k, v) in ctr_sorted]\n",
    "\n",
    "    keys = ['_'.join(x.split('_')[2:]) for x in keys]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.bar(keys, values, zorder=2)\n",
    "    plt.xticks(rotation='vertical')\n",
    "\n",
    "    plt.title(f\"Feature occurance using {n_features} features with scelector {selector}\")\n",
    "    plt.grid(axis='y', zorder=0)\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.yticks(np.arange(0, 101 + 1, step=5))\n",
    "    plt.ylabel(\"Occurances\")\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex3_anova\")\n",
    "plot_importances(anova_table_df, \"ANOVA\", n_features=20, save_path=save_path, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex3_rfe\")\n",
    "plot_importances(rfe_table_df, \"RFE\", n_features=15, save_path=save_path, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(DATA_PATH, \"experiments\", f\"ex3_sfs\")\n",
    "plot_importances(anova_table_df, \"SFS\", n_features=22, save_path=save_path, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8297a13c7e1935234816337df410bd40b701623c689ef22122c3456c516adc57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('face')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
